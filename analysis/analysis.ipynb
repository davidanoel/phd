{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<b>PhD Dissertation Report</b>\n",
    "<br><br>\n",
    "David Noel<br>\n",
    "Nova Southeastern University<br>\n",
    "CISD 901: Doctoral Dissertation<br><br>\n",
    "May, 2023\n",
    "<br>\n",
    "<br>\n",
    "<b>Committee:</b> Dr. Sumitra Mukherjee (Chair), Dr. Michael Laszlo, Dr. Frank Mitropoulos\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Spatial Invariance in Convolutional Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tDoes data augmentation lead to more spatially invariant and robust networks?\n",
    "2.\tAre deep architectures more spatially invariant than wide architectures? Did a particular architecture perform better?\n",
    "3.\tDoes generative data augmentation lead to more diverse samples than reinforcement learning techniques? Policy 2 vs. Policy 3.\n",
    "4.\tDo larger datasets increase spatial invariance? (Policy 2,4,5 vs. Policy 1 and 3)\n",
    "5.\tDo stacking data augmentation methods lead to more robust networks? Policy 5.\n",
    "6.\tDoes combining synthesized samples with original unaugmented samples improve invariance in CNNs? Policy 4\n",
    "7.\tDo synthesized samples improve invariance? Policy 3\n",
    "8.\tDo Stochastic techniques based on Reinforcement learning techniques improve invariance? Policy 2\n",
    "9.\tDoes the number of parameters in a CNN affect its spatial invariance as measured by its test accuracy?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"cifar10\" # \"cifar10\", \"mnist\", \"fmnist\n",
    "NETWORK = \"basic\"  # \"resnet\", \"inception\", \"basic\"\n",
    "FOLDER = \".\" + os.sep + DATASET + os.sep + NETWORK\n",
    "HISTORY_FILE = FOLDER + os.sep + f'{DATASET}_{NETWORK}_history.csv'\n",
    "METRICS_FILE = FOLDER + os.sep + f'{DATASET}_{NETWORK}_metrics.csv'\n",
    "NUMBER_POLICIES = 5\n",
    "NETWORK_TITLE = \"ResNet-50\" if NETWORK == \"resnet\" else \"InceptionV3\" if NETWORK == \"inception\" else \"the Base Model\"\n",
    "DATASET_TITLE = DATASET.upper()\n",
    "NUM_EXPERIMENTS = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert DATASET in [\"cifar10\", \"mnist\", \"fmnist\"], \"DATASET must be cifar10, mnist or fmnist\"\n",
    "assert NETWORK in [\"resnet\", \"inception\", \"basic\"], \"NETWORK must be resnet, inception, or basic\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load history and metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(HISTORY_FILE)\n",
    "print(\"Total records:\", len(history))\n",
    "history.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all metrics files into pandas dataframe\n",
    "all_metric_files = Path(\"./\").rglob('*metrics.csv')\n",
    "metrics = pd.concat((pd.read_csv(f) for f in all_metric_files), ignore_index=True)\n",
    "assert len(metrics) == NUM_EXPERIMENTS, \"Number of experiments must be 45\"\n",
    "metrics.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Network Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metrics.where((metrics['dataset'] == DATASET) & (metrics['model']==NETWORK)).dropna()\n",
    "df['policy'] = df['policy'].astype(int)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percent improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute percentate change in accuracy relative to baseline\n",
    "df['delta(%)'] = df.groupby('model')['accuracy'].apply(lambda x: x.div(x.iloc[0]).sub(1).mul(100)).round(2)\n",
    "\n",
    "# set decimal places\n",
    "df.precision = df.precision.round(4)\n",
    "df.recall = df.recall.round(4)\n",
    "df.f1score = df.f1score.round(4)\n",
    "df.latency = df.latency.round(2)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(FOLDER + \"/{}_{}_pct_change.csv\".format(DATASET, NETWORK), index=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epoch summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show number of epochs for each policy\n",
    "epochs = history.groupby('policy').count().rename(columns={'loss': 'epochs'})\n",
    "epochs['dataset'] = DATASET\n",
    "epochs['model'] = NETWORK\n",
    "epochs = epochs.reset_index()[['dataset','model','policy', 'epochs']]\n",
    "epochs.to_csv(FOLDER + \"/{}_{}_epochs.csv\".format(DATASET, NETWORK), index=False)\n",
    "epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and plot policy histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max loss in history. will be used to set y-axis limit in loss plot\n",
    "max_loss = max(history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the history for each policy\n",
    "policies = []\n",
    "for i in range(1, NUMBER_POLICIES + 1):\n",
    "    policies.append(history[history['policy'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, policy_num):\n",
    "    epochs = range(1, len(history.loss) + 1)\n",
    "    # plot loss during training\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(f\"Policy {policy_num} for {DATASET_TITLE} on {NETWORK_TITLE}\")\n",
    "    ax1.set_title(\"Model Loss\", fontsize=10)\n",
    "    ax1.plot(epochs, history.loss, \"--\")\n",
    "    ax1.plot(epochs, history.val_loss, \"--\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_ylim(top=max_loss + 0.1)\n",
    "    ax1.legend([\"training\", \"validation\"], loc=\"best\")\n",
    "    # plot accuracy during training\n",
    "    ax2.set_title(\"Model Accuracy\", fontsize=10)\n",
    "    ax2.plot(epochs, history.accuracy, \"--\")\n",
    "    ax2.plot(epochs, history.val_accuracy, \"--\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.legend([\"training\", \"validation\"], loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FOLDER + f\"/{DATASET}_{NETWORK}_policy{policy_num}_plot.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history for each policy\n",
    "for i, p in enumerate(policies):\n",
    "    plot_history(p, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_policies(policies):\n",
    "    #epochs = range(1, len(policies[0].loss) + 1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(10, 6))\n",
    "    fig.suptitle(f\"All policies for {DATASET_TITLE} on {NETWORK_TITLE}\")\n",
    "    ax1.set_title(\"Training Loss\", fontsize=10)\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(\"Validation Accuracy\", fontsize=10)\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    for i, p in enumerate(policies):\n",
    "        epochs = range(1, len(p.loss) + 1)\n",
    "        ax1.plot(epochs, p.loss, \"--\", label=f\"policy {i+1}\")\n",
    "        ax2.plot(epochs, p.val_accuracy, label=f\"policy {i+1}\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FOLDER + \"/{}_{}_all_policies_plot.png\".format(DATASET, NETWORK))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history for all policies\n",
    "plot_all_policies(policies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latency\n",
    "#### Training latency per policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot latency for DATASET and NETWORK\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(df.policy, df.latency)\n",
    "ax.bar_label(bars, fmt=\"%.0f\", padding=1, label_type=\"center\")\n",
    "ax.set_title(f\"Training latency for {DATASET_TITLE} on {NETWORK_TITLE}\")\n",
    "ax.set_xlabel(\"Latency (seconds)\")\n",
    "ax.set_ylabel(\"Policy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total training latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get time delta from seconds\n",
    "def get_time(num_seconds):\n",
    "    h, m, s = str(datetime.timedelta(seconds=int(num_seconds))).split(':')\n",
    "    return f'{h} hours, {m} minutes and {s} seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute total training latency in hours, minutes and seconds\n",
    "train_time = metrics.pivot_table(index='model', columns='dataset', values='latency', aggfunc='sum')\n",
    "\n",
    "print(\"BASIC MODEL TRAINING TIME\")\n",
    "seconds = 0\n",
    "for i in train_time.columns:\n",
    "    seconds += train_time[i].basic\n",
    "    time_delta = get_time(train_time[i].basic)\n",
    "    print(f'{i}: {time_delta}')\n",
    "print(\"Total train time: \", get_time(seconds))\n",
    "\n",
    "print(\"\\nINCEPTION TRAINING TIME\")\n",
    "seconds = 0\n",
    "for i in train_time.columns:\n",
    "    seconds += train_time[i].inception\n",
    "    time_delta = get_time(train_time[i].inception)\n",
    "    print(f'{i}: {time_delta}')\n",
    "print(\"Total train time: \", get_time(seconds))\n",
    "\n",
    "print(\"\\nRESNET TRAINING TIME\")\n",
    "seconds = 0\n",
    "for i in train_time.columns:\n",
    "    seconds += train_time[i].resnet\n",
    "    time_delta = get_time(train_time[i].resnet)\n",
    "    print(f'{i}: {time_delta}')\n",
    "print(\"Total train time: \", get_time(seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.barplot(x = 'model',\n",
    "            y = 'latency',\n",
    "            hue = 'dataset',\n",
    "            data = metrics,\n",
    "            palette = \"Blues\",\n",
    "            errwidth=0,\n",
    "            estimator=sum,\n",
    "            edgecolor = \"w\")\n",
    "ax.set_title(\"Training Times Across Different Models\")\n",
    "ax.set(xlabel='Model', ylabel='Latency (seconds)')\n",
    "ax.legend(title=\"Dataset\",loc='best')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,fmt=\"%.0f\", padding=1,label_type='center')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy and F-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar1 = ax.bar(df.policy - 0.2, df.accuracy, 0.4, label = 'Accuracy')\n",
    "bar2 = ax.bar(df.policy + 0.2, df.f1score, 0.4, label = 'F-1 score',color=sns.color_palette(\"Blues\")[1])\n",
    "ax.bar_label(bar1, fmt=\"%.4f\", padding=1, label_type='center')\n",
    "ax.bar_label(bar2, fmt=\"%.4f\", padding=1, label_type='center')\n",
    "ax.set_xlabel(\"Policy\")\n",
    "ax.set_ylabel(\"Accuracy/F-1 score\")\n",
    "ax.set_title(f\"Accuracy and F-1 score for {DATASET_TITLE} on {NETWORK_TITLE}\")\n",
    "ax.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group metrics by highest accuracy\n",
    "max_acc = metrics.groupby(['dataset','model'])['accuracy'].max().reset_index()\n",
    "max_acc = max_acc.merge(metrics, on=['dataset','model','accuracy'])\n",
    "max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show chart of highest accuracy for each dataset and model\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.barplot(x = 'model',\n",
    "            y = 'accuracy',\n",
    "            hue = 'dataset',\n",
    "            data = max_acc,\n",
    "            palette = \"Blues\",\n",
    "            errwidth=0,\n",
    "            edgecolor = \"w\")\n",
    "ax.set_title(\"Dataset Accuracy on Each Model\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,fmt=\"%.4f\", padding=1,label_type='center')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Accuracy on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.barplot(x = 'dataset',\n",
    "            y = 'accuracy',\n",
    "            hue = 'policy',\n",
    "            data = metrics,\n",
    "            palette = \"Blues\",\n",
    "            errwidth=0,\n",
    "            edgecolor = \"w\")\n",
    "ax.set_title(\"Policy Accuracy on Each Dataset\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,fmt=\"%.4f\", padding=1,label_type='center',rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Accuracy on Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.barplot(x = 'model',\n",
    "            y = 'accuracy',\n",
    "            hue = 'policy',\n",
    "            data = metrics,\n",
    "            palette = \"Blues\",\n",
    "            errwidth=0,\n",
    "            edgecolor = \"w\")\n",
    "ax.set_title(\"Policy Accuracy on Each Model\")\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,fmt=\"%.4f\", padding=1, label_type='center',rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latency per Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average latency for each policy\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "sns.barplot(x = 'policy',\n",
    "            y = 'latency',\n",
    "            data = metrics,\n",
    "            palette = \"Blues\",\n",
    "            errwidth=0,\n",
    "            edgecolor = \"w\")\n",
    "ax.set_title(\"Average Latency for Each Policy\")\n",
    "ax.set(xlabel='policy', ylabel='latency (seconds)')\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i,fmt=\"%.0f\", padding=1,label_type='center')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy based on number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_stats = metrics.groupby(['model','params'])['accuracy'].describe()\n",
    "param_stats = param_stats.drop(['count'],axis=1)\n",
    "param_stats['mean'] = param_stats['mean'].round(4)\n",
    "param_stats['std'] = param_stats['std'].round(4)\n",
    "param_stats['min'] = param_stats['min'].round(4)\n",
    "param_stats['max'] = param_stats['max'].round(4)\n",
    "param_stats = param_stats.rename(columns={'50%': 'median'}).round(4)\n",
    "param_stats = param_stats.reset_index()\n",
    "param_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax = sns.boxplot(x=metrics.model, y=metrics.accuracy, hue=metrics.params, data=metrics, palette=\"Blues\")\n",
    "ax.set_title(\"Accuracy per Total Parameters\")\n",
    "ax.set_xlabel(\"model\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.legend(title=\"Total Parameters\",loc='best')\n",
    "\n",
    "lines = ax.get_lines()\n",
    "categories = ax.get_xticks()\n",
    "\n",
    "for cat in categories:\n",
    "    y = lines[4+cat*6].get_ydata()[0]\n",
    "    ax.text(\n",
    "        cat, \n",
    "        y, \n",
    "        f'{y}', \n",
    "        ha='center', \n",
    "        va='center', \n",
    "        color='white',\n",
    "        bbox=dict(facecolor='#828282', edgecolor='#828282')\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
